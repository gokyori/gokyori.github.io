---
title: " The Behavior of Large Language Models When Prompted to Generate Code Explanations!"
collection: publications
permalink: /publications/codeexplanations_llm.md
excerpt: ""
date: 2023-11-06
venue: "NeurIPS'23 GAIED"
classes: wide
citation: "Priti Oli, Rabin Banjade, Jeevan Chapagain, Vasile Rus"
link: ""
paperurl: ""
---

## Summary

## Contribution

## Abstract

This paper systematically investigates the generation of code explanations by Large Language Models (LLMs) for code examples commonly encountered in introductory programming courses. Our findings reveal significant variations in the nature of code explanations produced by LLMs, influenced by factors such as the wording of the prompt, the specific code examples under consideration, the programming language involved, the temperature parameter, and the version of the LLM. However, a consistent pattern emerges for Java and Python, where explanations exhibit a Flesch-Kincaid readability level of approximately 7-8 grade and a consistent lexical density, indicating the proportion of meaningful words relative to the total explanation size. Additionally, the generated explanations consistently achieve high scores for correctness, but lower scores on three other metrics: completeness, conciseness, and specificity.
## Recommended citation:
Priti Oli, Rabin Banjade, Jeevan Chapagain, Vasile Rus. "The Behavior of Large Language Models When Prompted to Generate Code Explanations." . In NeurIPS'23 Workshop on Generative AI for Education (GAIED).2023.
